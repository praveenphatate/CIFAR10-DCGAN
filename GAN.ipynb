{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions.normal import Normal\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "batchSize = 64 \n",
    "image_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                ])\n",
    "dataset = dset.CIFAR10(root = './data', download = True, transform = transform) \n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) \n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 3, 4, 2, 1),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Tanh(),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "Tensor = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "#fixed_noise = np.random.normal(loc=0.0,scale=1, size = (batchSize, 100, 1, 1))\n",
    "#print(fixed_noise)\n",
    "#fixed_noise = torch.FloatTensor(fixed_noise).to(device)\n",
    "\n",
    "\n",
    "fixed_noise = torch.randn(batchSize, 100, 1, 1).to(device)\n",
    "#print(fixed_noise)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader,netD,netG,criterion, fixed_noise, real_label, fake_label):\n",
    "    #D_x = 0\n",
    "    #D_G_z1 = 0\n",
    "    #D_G_z2 = 0\n",
    "    #corrects = 0\n",
    "    for epoch in range(50):\n",
    "        D_x = 0\n",
    "        D_G_z1 = 0\n",
    "        D_G_z2 = 0\n",
    "        corrects = 0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            # train with real\n",
    "            netD.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            batch_size = real_cpu.size(0)\n",
    "            label = torch.full((batch_size,), real_label).to(device)\n",
    "\n",
    "            output = netD(real_cpu)\n",
    "            errD_real = criterion(output, label)\n",
    "            errD_real.backward()\n",
    "            D_x += output.mean().item()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            # train with fake\n",
    "            noise = torch.randn(batch_size, 100, 1, 1).to(device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = netD(fake.detach())\n",
    "            errD_fake = criterion(output, label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 += output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label).to(device)  # fake labels are real for generator cost\n",
    "            output = netD(fake)\n",
    "            corrects += (output == label).sum().item()\n",
    "            \n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 += output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "            \n",
    "\n",
    "            if i % 100 == 0:\n",
    "                save_image(real_cpu,\n",
    "                        '../results/val/real_samples{}_{}.png'.format(epoch,i),\n",
    "                        normalize=True)\n",
    "                fake = netG(fixed_noise)\n",
    "                save_image(fake.detach(),\n",
    "                        '../results/val/fake_samples_epoch{}_{}.png'.format(epoch, i),\n",
    "                        normalize=True)\n",
    "        print('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, 25,\n",
    "                     errD.item(), errG.item(), D_x/len(dataloader), D_G_z1/len(dataloader), D_G_z2/len(dataloader)))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25] Loss_D: 0.5533 Loss_G: 1.8133 D(x): 0.9211 D(G(z)): 0.0784 / 0.0382\n",
      "[1/25] Loss_D: 0.2995 Loss_G: 3.5792 D(x): 0.9293 D(G(z)): 0.0707 / 0.0340\n",
      "[2/25] Loss_D: 0.0583 Loss_G: 5.6495 D(x): 0.9370 D(G(z)): 0.0630 / 0.0295\n",
      "[3/25] Loss_D: 0.1039 Loss_G: 5.8079 D(x): 0.9193 D(G(z)): 0.0806 / 0.0410\n",
      "[4/25] Loss_D: 0.2752 Loss_G: 8.1090 D(x): 0.9257 D(G(z)): 0.0745 / 0.0365\n",
      "[5/25] Loss_D: 0.1766 Loss_G: 8.1754 D(x): 0.9313 D(G(z)): 0.0686 / 0.0333\n",
      "[6/25] Loss_D: 0.2472 Loss_G: 5.3084 D(x): 0.9343 D(G(z)): 0.0653 / 0.0320\n",
      "[7/25] Loss_D: 0.0778 Loss_G: 8.6404 D(x): 0.9450 D(G(z)): 0.0549 / 0.0243\n",
      "[8/25] Loss_D: 0.1056 Loss_G: 6.0366 D(x): 0.9242 D(G(z)): 0.0757 / 0.0366\n",
      "[9/25] Loss_D: 1.3089 Loss_G: 22.5661 D(x): 0.9529 D(G(z)): 0.0473 / 0.0203\n",
      "[10/25] Loss_D: 0.0191 Loss_G: 6.6133 D(x): 0.9226 D(G(z)): 0.0771 / 0.0380\n",
      "[11/25] Loss_D: 0.1874 Loss_G: 4.8512 D(x): 0.9310 D(G(z)): 0.0689 / 0.0342\n",
      "[12/25] Loss_D: 0.3060 Loss_G: 5.2900 D(x): 0.9342 D(G(z)): 0.0657 / 0.0319\n",
      "[13/25] Loss_D: 0.3205 Loss_G: 5.3652 D(x): 0.9370 D(G(z)): 0.0628 / 0.0310\n",
      "[14/25] Loss_D: 0.3887 Loss_G: 3.3420 D(x): 0.9372 D(G(z)): 0.0626 / 0.0293\n",
      "[15/25] Loss_D: 0.0376 Loss_G: 5.7714 D(x): 0.9463 D(G(z)): 0.0537 / 0.0232\n",
      "[16/25] Loss_D: 0.1137 Loss_G: 8.5894 D(x): 0.9440 D(G(z)): 0.0560 / 0.0237\n",
      "[17/25] Loss_D: 0.0390 Loss_G: 6.5373 D(x): 0.9224 D(G(z)): 0.0778 / 0.0382\n",
      "[18/25] Loss_D: 0.0729 Loss_G: 7.2571 D(x): 0.9445 D(G(z)): 0.0553 / 0.0255\n",
      "[19/25] Loss_D: 0.1335 Loss_G: 5.4060 D(x): 0.9299 D(G(z)): 0.0703 / 0.0357\n",
      "[20/25] Loss_D: 1.4707 Loss_G: 9.6276 D(x): 0.9538 D(G(z)): 0.0464 / 0.0253\n",
      "[21/25] Loss_D: 0.0777 Loss_G: 7.7528 D(x): 0.9383 D(G(z)): 0.0614 / 0.0274\n",
      "[22/25] Loss_D: 0.2888 Loss_G: 4.6610 D(x): 0.9390 D(G(z)): 0.0606 / 0.0300\n",
      "[23/25] Loss_D: 0.2196 Loss_G: 6.3137 D(x): 0.9409 D(G(z)): 0.0593 / 0.0296\n",
      "[24/25] Loss_D: 0.1443 Loss_G: 7.4555 D(x): 0.9563 D(G(z)): 0.0435 / 0.0187\n",
      "[25/25] Loss_D: 0.5222 Loss_G: 2.5501 D(x): 0.9297 D(G(z)): 0.0700 / 0.0344\n",
      "[26/25] Loss_D: 0.0905 Loss_G: 7.8211 D(x): 0.9520 D(G(z)): 0.0483 / 0.0225\n",
      "[27/25] Loss_D: 0.6361 Loss_G: 0.8064 D(x): 0.9389 D(G(z)): 0.0605 / 0.0295\n",
      "[28/25] Loss_D: 0.1171 Loss_G: 4.8094 D(x): 0.9543 D(G(z)): 0.0459 / 0.0197\n",
      "[29/25] Loss_D: 0.0816 Loss_G: 5.0507 D(x): 0.9353 D(G(z)): 0.0647 / 0.0314\n",
      "[30/25] Loss_D: 0.4203 Loss_G: 11.3537 D(x): 0.9452 D(G(z)): 0.0549 / 0.0261\n",
      "[31/25] Loss_D: 0.0789 Loss_G: 8.3201 D(x): 0.9417 D(G(z)): 0.0583 / 0.0282\n",
      "[32/25] Loss_D: 0.1282 Loss_G: 6.0872 D(x): 0.9593 D(G(z)): 0.0404 / 0.0169\n",
      "[33/25] Loss_D: 0.0200 Loss_G: 6.5614 D(x): 0.9291 D(G(z)): 0.0708 / 0.0347\n",
      "[34/25] Loss_D: 0.0116 Loss_G: 7.6921 D(x): 0.9592 D(G(z)): 0.0408 / 0.0182\n",
      "[35/25] Loss_D: 0.1065 Loss_G: 7.0759 D(x): 0.9476 D(G(z)): 0.0523 / 0.0247\n",
      "[36/25] Loss_D: 0.5783 Loss_G: 2.5946 D(x): 0.9405 D(G(z)): 0.0591 / 0.0278\n",
      "[37/25] Loss_D: 0.0492 Loss_G: 5.4470 D(x): 0.9690 D(G(z)): 0.0311 / 0.0128\n",
      "[38/25] Loss_D: 0.1153 Loss_G: 9.7469 D(x): 0.9416 D(G(z)): 0.0585 / 0.0271\n",
      "[39/25] Loss_D: 0.0747 Loss_G: 5.6333 D(x): 0.9395 D(G(z)): 0.0611 / 0.0313\n",
      "[40/25] Loss_D: 0.0896 Loss_G: 7.7297 D(x): 0.9629 D(G(z)): 0.0369 / 0.0155\n",
      "[41/25] Loss_D: 0.1297 Loss_G: 5.3994 D(x): 0.9444 D(G(z)): 0.0557 / 0.0265\n",
      "[42/25] Loss_D: 0.1058 Loss_G: 5.4937 D(x): 0.9673 D(G(z)): 0.0326 / 0.0139\n",
      "[43/25] Loss_D: 0.3943 Loss_G: 4.1783 D(x): 0.9356 D(G(z)): 0.0644 / 0.0309\n",
      "[44/25] Loss_D: 0.0713 Loss_G: 6.7153 D(x): 0.9569 D(G(z)): 0.0432 / 0.0194\n",
      "[45/25] Loss_D: 0.0695 Loss_G: 6.8348 D(x): 0.9508 D(G(z)): 0.0493 / 0.0236\n",
      "[46/25] Loss_D: 0.0581 Loss_G: 4.9734 D(x): 0.9664 D(G(z)): 0.0334 / 0.0139\n",
      "[47/25] Loss_D: 0.0213 Loss_G: 6.2244 D(x): 0.9339 D(G(z)): 0.0662 / 0.0322\n",
      "[48/25] Loss_D: 0.9696 Loss_G: 18.5199 D(x): 0.9623 D(G(z)): 0.0379 / 0.0171\n",
      "[49/25] Loss_D: 0.3475 Loss_G: 6.7305 D(x): 0.9416 D(G(z)): 0.0580 / 0.0288\n"
     ]
    }
   ],
   "source": [
    "train_model(dataloader,netD,netG,criterion, fixed_noise, real_label, fake_label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
